{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network tester\n",
    "- Load a pre-trained NN and test it against 1296 passwords\n",
    "- Save a CSV with the hystory and cuts (it could be used by *matches_analyzer.ipynb*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRINT_INFO = False\n",
    "\n",
    "NETWORKS_FAMILY_PATH = \"../4_training/trained/hopeful/model_20191026_1947\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Networks to test:['model_20191026_1947_0', 'model_20191026_1947_1', 'model_20191026_1947_2', 'model_20191026_1947_3']\n",
      "Network(s) name: model_20191026_1947\n"
     ]
    }
   ],
   "source": [
    "networks_name = os.path.basename(NETWORKS_FAMILY_PATH)\n",
    "networks_paths = os.listdir(NETWORKS_FAMILY_PATH)\n",
    "assert len(networks_paths) == 4\n",
    "networks_paths.sort(key= lambda name: int(name[-1]))\n",
    "print(\"Networks to test:{}\".format(networks_paths))\n",
    "\n",
    "MODELS_PATH = []\n",
    "INFOS_PATH = []\n",
    "for network_path in networks_paths:\n",
    "    model_path = NETWORKS_FAMILY_PATH+\"/\"+network_path+\"/export/predict/\"\n",
    "    info_path = NETWORKS_FAMILY_PATH+\"/\"+network_path+\"/model.info\"\n",
    "    export_id = os.listdir(model_path)\n",
    "    model_path += export_id[0]\n",
    "    MODELS_PATH.append(model_path)\n",
    "    INFOS_PATH.append(info_path)\n",
    "print('Network(s) name: '+networks_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------\n",
    "## Load the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info network 0:{\n",
      "    \"ALPHA\": 0.001,\n",
      "    \"ATTENTION_SIZE\": 256,\n",
      "    \"BATCH_EVAL\": 32,\n",
      "    \"BATCH_TRAIN\": 32,\n",
      "    \"BW_FORGET_BIAS\": 1,\n",
      "    \"CSV_PATH_EVAL\": \"../2_database/dbML/hopeful/hopeful_2019-10-05_0.8_206957_eval.csv\",\n",
      "    \"CSV_PATH_TRAIN\": \"../2_database/dbML/hopeful/hopeful_2019-10-05_0.8_829843_train.csv\",\n",
      "    \"DROP_REPRESENTATION\": 0.3,\n",
      "    \"EPOCHS\": 10,\n",
      "    \"EVAL_STEPS\": null,\n",
      "    \"FW_FORGET_BIAS\": 1,\n",
      "    \"GOAL\": \"goalReduced\",\n",
      "    \"GOAL_POS\": 0,\n",
      "    \"INPUT_ENCODING\": \"Peg: Hot encoding, Tips: One Hot Encoding\",\n",
      "    \"LSTM_SIZE\": 256,\n",
      "    \"MAXOUT_SIZE\": 256,\n",
      "    \"MODEL_DIR_PATH\": \"model_20191026_1947_0\",\n",
      "    \"MULTI_THREADING\": true,\n",
      "    \"NUM_EVALS\": 10,\n",
      "    \"SHUFFLE_EVAL\": false,\n",
      "    \"_NETWORK_TYPE\": \"BiLSTM_AttentionSystem\",\n",
      "    \"loss_fn\": \"mean_pairwise_squared_error\",\n",
      "    \"train_duration\": \"3:08:47\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(INFOS_PATH[0]) as f:\n",
    "    nn_info = json.load(f)\n",
    "    print(\"Info network 0:\"+json.dumps(nn_info, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/zimmy/envp3/lib/python3.6/site-packages/tensorflow/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /home/zimmy/envp3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ../4_training/trained/hopeful/model_20191026_1947/model_20191026_1947_0/export/predict/1572123393/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from ../4_training/trained/hopeful/model_20191026_1947/model_20191026_1947_1/export/predict/1572134988/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from ../4_training/trained/hopeful/model_20191026_1947/model_20191026_1947_2/export/predict/1572145953/variables/variables\n",
      "INFO:tensorflow:Restoring parameters from ../4_training/trained/hopeful/model_20191026_1947/model_20191026_1947_3/export/predict/1572156854/variables/variables\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_path):\n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "                            export_dir = model_path,\n",
    "                            signature_def_key=\"prediction\")\n",
    "    return predictor_fn\n",
    "\n",
    "models = []\n",
    "for path in MODELS_PATH:\n",
    "    model = load_model(path)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "## Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from string import ascii_uppercase\n",
    "FEATURES_NAME = ['Guess 1', 'Guess 2', 'Guess 3', 'Guess 4', 'Guess 5',\n",
    "          'Guess 6', 'Guess 7', 'Guess 8', 'Guess 9', 'Guess 10']\n",
    "\n",
    "TARGET_NAME = 'PASSWORD'\n",
    "\n",
    "HEADER = FEATURES_NAME + [TARGET_NAME]\n",
    "\n",
    "HOT_ENCODING = {\n",
    "    \"100000\" : 'A',\n",
    "    \"010000\" : 'B',\n",
    "    \"001000\" : 'C',\n",
    "    \"000100\" : 'D',\n",
    "    \"000010\" : 'E',\n",
    "    \"000001\" : 'F',\n",
    "}\n",
    "\n",
    "MATCH_LOST_PAD = 'XXXX'\n",
    "\n",
    "# Classic Mastermind rules\n",
    "PSW_CARDINALITY = 6\n",
    "PSW_LEN = 4\n",
    "PSW_POOL_COMPLETE = [''.join(p) for p in product(ascii_uppercase[0:PSW_CARDINALITY], repeat=PSW_LEN)]\n",
    "\n",
    "# Analysis persistance\n",
    "def save_to_csv(matches, tail_name):\n",
    "    df = pd.DataFrame(data=matches, columns=HEADER)\n",
    "    with open('./matchesPlayed/{}_{}.csv'.format(networks_name, tail_name),'w') as f:\n",
    "        df.to_csv(f, sep=',', encoding='utf-8',index=False, header=True, float_format=\"%.0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_char(s, p, r):\n",
    "    return s[:p]+r+s[p+1:]\n",
    "\n",
    "def hints_calculator(psw_goal, psw_guess):\n",
    "    black_pegs = len([goal_element for goal_element, guess_element in zip(psw_goal, psw_guess) if goal_element == guess_element])\n",
    "    white_pegs = sum([min(psw_goal.count(element), psw_guess.count(element)) for element in set('ABCDEF')]) - black_pegs\n",
    "    return black_pegs, white_pegs\n",
    "\n",
    "def get_clean_match():\n",
    "    match = {}\n",
    "    cuts = {}\n",
    "    for guess in FEATURES_NAME:\n",
    "        match[guess] = [\"<pad>\"]\n",
    "        cuts[guess] = 0\n",
    "    match[TARGET_NAME] = [\"----\"]\n",
    "    cuts[TARGET_NAME] = \"----\"\n",
    "    return match, cuts\n",
    "\n",
    "def make_a_guess(match, psw):\n",
    "    for idx, model in enumerate(models):\n",
    "        prediction = model(match)['Prediction'][0]\n",
    "        prediction = \"\".join(str(int(digit)) for digit in prediction)\n",
    "        prediction = HOT_ENCODING[prediction]\n",
    "        match[TARGET_NAME][0] = change_char(match[TARGET_NAME][0], idx, prediction)\n",
    "    blk_peg, wht_peg = hints_calculator(psw, match[TARGET_NAME][0])\n",
    "    guess = \"{}{}{}\".format(match[TARGET_NAME][0], blk_peg, wht_peg)\n",
    "    match[TARGET_NAME][0] = '----'\n",
    "    won = blk_peg == 4\n",
    "    return guess, won, (blk_peg, wht_peg)\n",
    "\n",
    "def guess_pool_pruning(psw_pool, next_guess, hints):\n",
    "    to_remove = []\n",
    "    for psw in psw_pool:\n",
    "        if hints != hints_calculator(psw, next_guess):\n",
    "            to_remove.append(psw)\n",
    "    len_psw_pool = len(psw_pool)\n",
    "    psw_pool.difference_update(to_remove)\n",
    "    return int(len_psw_pool - len(psw_pool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "victories = 0\n",
    "matches = []\n",
    "cuts_hist = []\n",
    "\n",
    "for PSW in PSW_POOL_COMPLETE:\n",
    "    won = False\n",
    "    psw_pool = set(PSW_POOL_COMPLETE.copy())\n",
    "    match, cuts = get_clean_match()\n",
    "    for guess_pos in FEATURES_NAME:\n",
    "        guess, is_psw, pegs = make_a_guess(match, PSW)\n",
    "        match[guess_pos] = [guess]\n",
    "        cuts[guess_pos] = guess_pool_pruning(psw_pool, guess[:-2], pegs)\n",
    "        if is_psw:\n",
    "            won = True\n",
    "            break\n",
    "            \n",
    "    match = {guess_pos: guess[0] for guess_pos, guess in match.items()}\n",
    "    match[TARGET_NAME] = PSW\n",
    "    cuts[TARGET_NAME] = PSW\n",
    "    if won:\n",
    "        victories = victories + 1\n",
    "    else:\n",
    "        match['Guess 10'] = MATCH_LOST_PAD\n",
    "        if PRINT_INFO:\n",
    "            print(\"Psw {} not guessed, match played:\\n\".format(PSW))\n",
    "            pprint(match)\n",
    "            print('\\n\\n')\n",
    "    matches.append(match)\n",
    "    cuts_hist.append(cuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(matches, 'play')\n",
    "save_to_csv(cuts_hist, 'play_cuts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victories: 1180 / 1296\n",
      "Ratio: 0.9104938271604939\n"
     ]
    }
   ],
   "source": [
    "print(\"Victories: {} / {}\".format(victories, len(PSW_POOL_COMPLETE)))\n",
    "print(\"Ratio: {}\".format(victories/len(PSW_POOL_COMPLETE)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
